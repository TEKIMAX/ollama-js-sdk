export interface Lesson {
  id: string;
  title: string;
  description: string;
  content: string;
  examples?: string[];
  quiz?: QuizQuestion[];
}

export interface QuizQuestion {
  question: string;
  options: string[];
  correctAnswer: number;
  explanation: string;
}

export interface Chapter {
  id: string;
  title: string;
  description: string;
  lessons: Lesson[];
}

export const AI_CURRICULUM: Chapter[] = [
  {
    id: 'fundamentals',
    title: 'ðŸŽ“ Chapter 1: AI Fundamentals',
    description: 'Understanding the basics of artificial intelligence and language models',
    lessons: [
      {
        id: 'what-is-llm',
        title: 'What is a Large Language Model (LLM)?',
        description: 'Introduction to LLMs and how they work',
        content: `
# Large Language Models (LLMs)

A Large Language Model is a type of artificial intelligence model that:

## Key Characteristics:
â€¢ **Size**: Contains billions of parameters (weights)
â€¢ **Training**: Learned from massive text datasets
â€¢ **Purpose**: Understands and generates human-like text
â€¢ **Architecture**: Based on transformer neural networks

## How LLMs Work:
1. **Input Processing**: Text is broken into tokens
2. **Pattern Recognition**: Model identifies patterns in the tokens
3. **Prediction**: Generates probable next tokens
4. **Output**: Combines tokens into coherent text

## GPT-OSS Models in This SDK:
â€¢ **GPT-OSS 20**: 20 billion parameters - Fast and efficient
â€¢ **GPT-OSS 120**: 120 billion parameters - More sophisticated responses
        `,
        examples: [
          'Input: "The capital of France is" â†’ Output: "Paris"',
          'Input: "2 + 2 equals" â†’ Output: "4"'
        ],
        quiz: [
          {
            question: 'What does LLM stand for?',
            options: ['Large Learning Machine', 'Large Language Model', 'Linear Language Model', 'Limited Language Model'],
            correctAnswer: 1,
            explanation: 'LLM stands for Large Language Model, which refers to AI models trained on vast amounts of text data.'
          }
        ]
      },
      {
        id: 'what-is-token',
        title: 'What is a Token?',
        description: 'Understanding tokenization in language models',
        content: `
# Tokens: The Building Blocks of Language Models

## Definition:
A **token** is the smallest unit of text that a language model processes.

## Types of Tokens:
â€¢ **Word tokens**: Complete words ("hello", "world")
â€¢ **Subword tokens**: Parts of words ("un", "believ", "able")
â€¢ **Character tokens**: Individual characters
â€¢ **Special tokens**: Punctuation, spaces, control characters

## Token Examples:
â€¢ "Hello, world!" â†’ ["Hello", ",", " world", "!"] (4 tokens)
â€¢ "unbelievable" â†’ ["un", "believ", "able"] (3 tokens)
â€¢ "GPT-OSS" â†’ ["GPT", "-", "OSS"] (3 tokens)

## Why Tokens Matter:
1. **Context Window**: Models have token limits (GPT-OSS 20: 8,192 tokens)
2. **Cost**: API pricing often based on token count
3. **Performance**: More tokens = more processing time
4. **Quality**: Token limits affect response length

## Token Estimation:
â€¢ English: ~1 token = 4 characters
â€¢ Rule of thumb: 1 token â‰ˆ 0.75 words
â€¢ 1,000 tokens â‰ˆ 750 words â‰ˆ 1.5 pages
        `,
        examples: [
          'Text: "AI is amazing" â†’ Tokens: ["AI", " is", " amazing"] (3 tokens)',
          'Text: "Temperature=0.7" â†’ Tokens: ["Temperature", "=", "0", ".", "7"] (5 tokens)'
        ]
      },
      {
        id: 'transformer-architecture',
        title: 'Transformer Architecture',
        description: 'The foundation of modern language models',
        content: `
# The Transformer Architecture

## Overview:
Transformers are the neural network architecture behind modern LLMs like GPT.

## Key Components:

### 1. Attention Mechanism
The "self-attention" mechanism allows the model to:
â€¢ Focus on relevant parts of the input
â€¢ Understand context and relationships
â€¢ Process sequences in parallel

### 2. Encoder-Decoder Structure
â€¢ **Encoder**: Processes and understands input
â€¢ **Decoder**: Generates output
â€¢ GPT models use decoder-only architecture

### 3. Multi-Head Attention
â€¢ Multiple attention mechanisms working in parallel
â€¢ Each "head" learns different relationships
â€¢ GPT-OSS 20: 32 attention heads
â€¢ GPT-OSS 120: 96 attention heads

### 4. Feed-Forward Networks
â€¢ Process information after attention
â€¢ Apply non-linear transformations
â€¢ Learn complex patterns

## Why Transformers are Revolutionary:
âœ“ **Parallel Processing**: Unlike RNNs, process all tokens simultaneously
âœ“ **Long-Range Dependencies**: Connect distant parts of text
âœ“ **Scalability**: Performance improves with size
âœ“ **Transfer Learning**: Pre-train once, fine-tune for many tasks
        `,
        quiz: [
          {
            question: 'What is the key innovation of transformer architecture?',
            options: ['Recurrent connections', 'Self-attention mechanism', 'Convolutional layers', 'Random forests'],
            correctAnswer: 1,
            explanation: 'The self-attention mechanism allows transformers to process sequences efficiently and understand context.'
          }
        ]
      }
    ]
  },
  {
    id: 'parameters',
    title: 'âš™ï¸ Chapter 2: Model Parameters',
    description: 'Understanding temperature, top-p, and other generation parameters',
    lessons: [
      {
        id: 'temperature',
        title: 'Temperature: Controlling Randomness',
        description: 'How temperature affects model creativity',
        content: `
# Temperature Parameter

## What is Temperature?
Temperature controls the randomness/creativity of model outputs.

## Temperature Scale:
â€¢ **0.0**: Deterministic, always picks most likely token
â€¢ **0.1-0.3**: Very focused, factual, consistent
â€¢ **0.4-0.7**: Balanced creativity and coherence (default: 0.7)
â€¢ **0.8-1.0**: Creative, varied, more random
â€¢ **1.0+**: Very random, potentially incoherent

## Use Cases by Temperature:

### Low Temperature (0.1-0.3):
âœ“ Code generation
âœ“ Factual Q&A
âœ“ Data extraction
âœ“ Technical documentation

### Medium Temperature (0.4-0.7):
âœ“ General conversation
âœ“ Email writing
âœ“ Summaries
âœ“ Explanations

### High Temperature (0.8-1.0):
âœ“ Creative writing
âœ“ Brainstorming
âœ“ Poetry
âœ“ Story generation

## Mathematical Explanation:
Temperature modifies the probability distribution:
â€¢ Lower T: Sharpens distribution (peaks become more pronounced)
â€¢ Higher T: Flattens distribution (more options become viable)
        `,
        examples: [
          'Prompt: "The sky is" with temp=0.1 â†’ "blue"',
          'Prompt: "The sky is" with temp=0.9 â†’ "dancing with colors"'
        ]
      },
      {
        id: 'context-window',
        title: 'Context Window',
        description: 'Understanding model memory limits',
        content: `
# Context Window

## Definition:
The context window is the maximum number of tokens a model can process at once.

## GPT-OSS Context Windows:
â€¢ **GPT-OSS 20**: 8,192 tokens (~6,000 words)
â€¢ **GPT-OSS 120**: 32,768 tokens (~24,000 words)

## What Fits in Context:

### GPT-OSS 20 (8K tokens):
â€¢ ~12 pages of text
â€¢ A short story
â€¢ Multiple code files
â€¢ Extended conversations

### GPT-OSS 120 (32K tokens):
â€¢ ~50 pages of text
â€¢ Small book chapters
â€¢ Complete codebases
â€¢ Long technical documents

## Context Window Strategies:

### 1. Chunking
Break large documents into smaller pieces

### 2. Summarization
Compress information to fit context

### 3. Sliding Window
Move context window through document

### 4. Hierarchical Processing
Process sections, then combine results

## Important Notes:
âš ï¸ Exceeding context = information loss
âš ï¸ Recent context has more influence
âš ï¸ Longer context = slower processing
        `
      },
      {
        id: 'top-p-top-k',
        title: 'Top-P and Top-K Sampling',
        description: 'Advanced sampling techniques',
        content: `
# Top-P (Nucleus) and Top-K Sampling

## Top-P (Nucleus Sampling):
Controls diversity by limiting cumulative probability.

### How it Works:
1. Order tokens by probability
2. Sum probabilities until reaching P threshold
3. Sample only from this "nucleus"

### Top-P Values:
â€¢ **0.1**: Very narrow selection (10% most likely)
â€¢ **0.5**: Moderate diversity (50% most likely)
â€¢ **0.9**: Wide selection (90% most likely)
â€¢ **1.0**: Consider all tokens

## Top-K Sampling:
Limits selection to K most likely tokens.

### How it Works:
1. Select K highest probability tokens
2. Redistribute probabilities
3. Sample from this subset

### Top-K Values:
â€¢ **1**: Always pick most likely (greedy)
â€¢ **10**: Very focused
â€¢ **40**: Balanced (default)
â€¢ **100**: More variety
â€¢ **âˆž**: No limit

## Combining Parameters:
Best results often come from combining:
â€¢ Temperature: 0.7
â€¢ Top-P: 0.9
â€¢ Top-K: 40

This provides controlled creativity without chaos!
        `,
        quiz: [
          {
            question: 'What does Top-P=0.9 mean?',
            options: [
              'Use 90 tokens',
              'Sample from tokens covering 90% cumulative probability',
              'Set temperature to 0.9',
              'Use 90% of context window'
            ],
            correctAnswer: 1,
            explanation: 'Top-P=0.9 means sampling from tokens that together represent 90% of the probability mass.'
          }
        ]
      }
    ]
  },
  {
    id: 'embeddings',
    title: 'ðŸ”¢ Chapter 3: Embeddings & Vectors',
    description: 'Understanding text embeddings and vector representations',
    lessons: [
      {
        id: 'what-are-embeddings',
        title: 'What are Embeddings?',
        description: 'Converting text to numbers',
        content: `
# Text Embeddings

## Definition:
Embeddings are numerical representations of text that capture semantic meaning.

## How Embeddings Work:

### 1. Text to Vector
"Hello world" â†’ [0.23, -0.45, 0.67, ..., 0.12]

### 2. Dimensions
â€¢ GPT-OSS 20: 4,096 dimensions
â€¢ GPT-OSS 120: 8,192 dimensions
â€¢ Each dimension captures different aspects

### 3. Semantic Similarity
Similar meanings = Similar vectors
â€¢ "dog" â‰ˆ "puppy" (high similarity)
â€¢ "dog" â‰  "car" (low similarity)

## Applications:

### Semantic Search
Find documents by meaning, not keywords

### Clustering
Group similar texts together

### Classification
Categorize text based on embeddings

### Recommendation
Find similar content

## Vector Operations:

### Cosine Similarity
Measures angle between vectors (0-1 scale)
â€¢ 1.0 = Identical meaning
â€¢ 0.5 = Somewhat related
â€¢ 0.0 = Unrelated

### Euclidean Distance
Measures straight-line distance
â€¢ Smaller = More similar
        `,
        examples: [
          'Text: "Machine learning" â†’ Vector: [0.34, -0.21, ...]',
          'Similarity("cat", "kitten") = 0.89',
          'Similarity("cat", "airplane") = 0.12'
        ]
      }
    ]
  },
  {
    id: 'fine-tuning',
    title: 'ðŸŽ¯ Chapter 4: Fine-Tuning & Training',
    description: 'Customizing models for specific tasks',
    lessons: [
      {
        id: 'fine-tuning-basics',
        title: 'Fine-Tuning Fundamentals',
        description: 'Adapting models to your needs',
        content: `
# Fine-Tuning Language Models

## What is Fine-Tuning?
Taking a pre-trained model and training it further on specific data.

## Why Fine-Tune?

### 1. Domain Specialization
Train on medical, legal, or technical texts

### 2. Task Optimization
Improve performance on specific tasks

### 3. Style Adaptation
Match writing style or tone

### 4. Knowledge Injection
Add proprietary or recent information

## Fine-Tuning Process:

### Step 1: Data Preparation
â€¢ Collect domain-specific examples
â€¢ Format as input-output pairs
â€¢ Clean and validate data

### Step 2: Configuration
â€¢ Learning rate: 1e-5 to 5e-5
â€¢ Batch size: Based on GPU memory
â€¢ Epochs: 3-5 typically

### Step 3: Training
â€¢ GPT-OSS 20: 2-4 hours on GPU
â€¢ GPT-OSS 120: 8-12 hours on GPU

### Step 4: Evaluation
â€¢ Test on validation set
â€¢ Compare to base model
â€¢ Check for overfitting

## Best Practices:
âœ“ Start with small learning rates
âœ“ Use at least 100 examples
âœ“ Monitor training loss
âœ“ Keep validation set separate
        `
      }
    ]
  },
  {
    id: 'prompting',
    title: 'âœï¸ Chapter 5: Prompt Engineering',
    description: 'Mastering the art of prompting',
    lessons: [
      {
        id: 'prompt-techniques',
        title: 'Advanced Prompting Techniques',
        description: 'Getting better results from LLMs',
        content: `
# Prompt Engineering Techniques

## 1. Zero-Shot Prompting
Direct instruction without examples:
"Translate 'Hello' to Spanish"

## 2. Few-Shot Prompting
Provide examples before the task:
\`\`\`
English: Hello â†’ Spanish: Hola
English: Goodbye â†’ Spanish: AdiÃ³s
English: Thank you â†’ Spanish: 
\`\`\`

## 3. Chain-of-Thought (CoT)
Ask model to explain reasoning:
"Let's solve this step by step..."

## 4. Role-Playing
Assign a persona:
"You are an expert Python developer..."

## 5. Structured Output
Request specific format:
"Respond in JSON format with keys: summary, sentiment, keywords"

## Prompt Components:

### Context
Background information

### Instruction
What you want done

### Input
The data to process

### Output Format
How to structure response

## Tips for Better Prompts:
âœ“ Be specific and clear
âœ“ Provide context
âœ“ Use examples
âœ“ Specify format
âœ“ Iterate and refine
        `,
        examples: [
          'Bad: "Write about dogs"',
          'Good: "Write a 200-word educational paragraph about golden retriever training techniques for first-time owners"'
        ]
      }
    ]
  }
];

export function getLessonById(chapterId: string, lessonId: string): Lesson | undefined {
  const chapter = AI_CURRICULUM.find(c => c.id === chapterId);
  return chapter?.lessons.find(l => l.id === lessonId);
}

export function searchLessons(query: string): Lesson[] {
  const results: Lesson[] = [];
  const searchTerm = query.toLowerCase();
  
  for (const chapter of AI_CURRICULUM) {
    for (const lesson of chapter.lessons) {
      if (
        lesson.title.toLowerCase().includes(searchTerm) ||
        lesson.description.toLowerCase().includes(searchTerm) ||
        lesson.content.toLowerCase().includes(searchTerm)
      ) {
        results.push(lesson);
      }
    }
  }
  
  return results;
}