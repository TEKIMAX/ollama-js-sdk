export interface Lesson {
  id: string;
  title: string;
  description: string;
  content: string;
  examples?: string[];
  quiz?: QuizQuestion[];
}

export interface QuizQuestion {
  question: string;
  options: string[];
  correctAnswer: number;
  explanation: string;
}

export interface Chapter {
  id: string;
  title: string;
  description: string;
  lessons: Lesson[];
}

export const AI_CURRICULUM: Chapter[] = [
  {
    id: 'fundamentals',
    title: 'üéì Chapter 1: AI Fundamentals',
    description: 'Understanding the basics of artificial intelligence and language models',
    lessons: [
      {
        id: 'what-is-llm',
        title: 'What is a Large Language Model (LLM)?',
        description: 'Introduction to LLMs and how they work',
        content: `
# Large Language Models (LLMs)

A Large Language Model is a type of artificial intelligence model that:

## Key Characteristics:
‚Ä¢ **Size**: Contains billions of parameters (weights)
‚Ä¢ **Training**: Learned from massive text datasets
‚Ä¢ **Purpose**: Understands and generates human-like text
‚Ä¢ **Architecture**: Based on transformer neural networks

## How LLMs Work:
1. **Input Processing**: Text is broken into tokens
2. **Pattern Recognition**: Model identifies patterns in the tokens
3. **Prediction**: Generates probable next tokens
4. **Output**: Combines tokens into coherent text

## GPT-OSS Models in This SDK:
‚Ä¢ **GPT-OSS 20**: 20 billion parameters - Fast and efficient
‚Ä¢ **GPT-OSS 120**: 120 billion parameters - More sophisticated responses
        `,
        examples: [
          'Input: "The capital of France is" ‚Üí Output: "Paris"',
          'Input: "2 + 2 equals" ‚Üí Output: "4"'
        ],
        quiz: [
          {
            question: 'What does LLM stand for?',
            options: ['Large Learning Machine', 'Large Language Model', 'Linear Language Model', 'Limited Language Model'],
            correctAnswer: 1,
            explanation: 'LLM stands for Large Language Model, which refers to AI models trained on vast amounts of text data.'
          }
        ]
      },
      {
        id: 'what-is-token',
        title: 'What is a Token?',
        description: 'Understanding tokenization in language models',
        content: `
# Tokens: The Building Blocks of Language Models

## Definition:
A **token** is the smallest unit of text that a language model processes.

## Types of Tokens:
‚Ä¢ **Word tokens**: Complete words ("hello", "world")
‚Ä¢ **Subword tokens**: Parts of words ("un", "believ", "able")
‚Ä¢ **Character tokens**: Individual characters
‚Ä¢ **Special tokens**: Punctuation, spaces, control characters

## Token Examples:
‚Ä¢ "Hello, world!" ‚Üí ["Hello", ",", " world", "!"] (4 tokens)
‚Ä¢ "unbelievable" ‚Üí ["un", "believ", "able"] (3 tokens)
‚Ä¢ "GPT-OSS" ‚Üí ["GPT", "-", "OSS"] (3 tokens)

## Why Tokens Matter:
1. **Context Window**: Models have token limits (GPT-OSS 20: 8,192 tokens)
2. **Cost**: API pricing often based on token count
3. **Performance**: More tokens = more processing time
4. **Quality**: Token limits affect response length

## Token Estimation:
‚Ä¢ English: ~1 token = 4 characters
‚Ä¢ Rule of thumb: 1 token ‚âà 0.75 words
‚Ä¢ 1,000 tokens ‚âà 750 words ‚âà 1.5 pages
        `,
        examples: [
          'Text: "AI is amazing" ‚Üí Tokens: ["AI", " is", " amazing"] (3 tokens)',
          'Text: "Temperature=0.7" ‚Üí Tokens: ["Temperature", "=", "0", ".", "7"] (5 tokens)'
        ]
      },
      {
        id: 'transformer-architecture',
        title: 'Transformer Architecture',
        description: 'The foundation of modern language models',
        content: `
# The Transformer Architecture

## Overview:
Transformers are the neural network architecture behind modern LLMs like GPT.

## Key Components:

### 1. Attention Mechanism
The "self-attention" mechanism allows the model to:
‚Ä¢ Focus on relevant parts of the input
‚Ä¢ Understand context and relationships
‚Ä¢ Process sequences in parallel

### 2. Encoder-Decoder Structure
‚Ä¢ **Encoder**: Processes and understands input
‚Ä¢ **Decoder**: Generates output
‚Ä¢ GPT models use decoder-only architecture

### 3. Multi-Head Attention
‚Ä¢ Multiple attention mechanisms working in parallel
‚Ä¢ Each "head" learns different relationships
‚Ä¢ GPT-OSS 20: 32 attention heads
‚Ä¢ GPT-OSS 120: 96 attention heads

### 4. Feed-Forward Networks
‚Ä¢ Process information after attention
‚Ä¢ Apply non-linear transformations
‚Ä¢ Learn complex patterns

## Why Transformers are Revolutionary:
‚úì **Parallel Processing**: Unlike RNNs, process all tokens simultaneously
‚úì **Long-Range Dependencies**: Connect distant parts of text
‚úì **Scalability**: Performance improves with size
‚úì **Transfer Learning**: Pre-train once, fine-tune for many tasks
        `,
        quiz: [
          {
            question: 'What is the key innovation of transformer architecture?',
            options: ['Recurrent connections', 'Self-attention mechanism', 'Convolutional layers', 'Random forests'],
            correctAnswer: 1,
            explanation: 'The self-attention mechanism allows transformers to process sequences efficiently and understand context.'
          }
        ]
      }
    ]
  },
  {
    id: 'parameters',
    title: '‚öôÔ∏è Chapter 2: Model Parameters',
    description: 'Understanding temperature, top-p, and other generation parameters',
    lessons: [
      {
        id: 'temperature',
        title: 'Temperature: Controlling Randomness',
        description: 'How temperature affects model creativity',
        content: `
# Temperature Parameter

## What is Temperature?
Temperature controls the randomness/creativity of model outputs.

## Temperature Scale:
‚Ä¢ **0.0**: Deterministic, always picks most likely token
‚Ä¢ **0.1-0.3**: Very focused, factual, consistent
‚Ä¢ **0.4-0.7**: Balanced creativity and coherence (default: 0.7)
‚Ä¢ **0.8-1.0**: Creative, varied, more random
‚Ä¢ **1.0+**: Very random, potentially incoherent

## Use Cases by Temperature:

### Low Temperature (0.1-0.3):
‚úì Code generation
‚úì Factual Q&A
‚úì Data extraction
‚úì Technical documentation

### Medium Temperature (0.4-0.7):
‚úì General conversation
‚úì Email writing
‚úì Summaries
‚úì Explanations

### High Temperature (0.8-1.0):
‚úì Creative writing
‚úì Brainstorming
‚úì Poetry
‚úì Story generation

## Mathematical Explanation:
Temperature modifies the probability distribution:
‚Ä¢ Lower T: Sharpens distribution (peaks become more pronounced)
‚Ä¢ Higher T: Flattens distribution (more options become viable)
        `,
        examples: [
          'Prompt: "The sky is" with temp=0.1 ‚Üí "blue"',
          'Prompt: "The sky is" with temp=0.9 ‚Üí "dancing with colors"'
        ]
      },
      {
        id: 'context-window',
        title: 'Context Window',
        description: 'Understanding model memory limits',
        content: `
# Context Window

## Definition:
The context window is the maximum number of tokens a model can process at once.

## GPT-OSS Context Windows:
‚Ä¢ **GPT-OSS 20**: 8,192 tokens (~6,000 words)
‚Ä¢ **GPT-OSS 120**: 32,768 tokens (~24,000 words)

## What Fits in Context:

### GPT-OSS 20 (8K tokens):
‚Ä¢ ~12 pages of text
‚Ä¢ A short story
‚Ä¢ Multiple code files
‚Ä¢ Extended conversations

### GPT-OSS 120 (32K tokens):
‚Ä¢ ~50 pages of text
‚Ä¢ Small book chapters
‚Ä¢ Complete codebases
‚Ä¢ Long technical documents

## Context Window Strategies:

### 1. Chunking
Break large documents into smaller pieces

### 2. Summarization
Compress information to fit context

### 3. Sliding Window
Move context window through document

### 4. Hierarchical Processing
Process sections, then combine results

## Important Notes:
‚ö†Ô∏è Exceeding context = information loss
‚ö†Ô∏è Recent context has more influence
‚ö†Ô∏è Longer context = slower processing
        `
      },
      {
        id: 'top-p-top-k',
        title: 'Top-P and Top-K Sampling',
        description: 'Advanced sampling techniques',
        content: `
# Top-P (Nucleus) and Top-K Sampling

## Top-P (Nucleus Sampling):
Controls diversity by limiting cumulative probability.

### How it Works:
1. Order tokens by probability
2. Sum probabilities until reaching P threshold
3. Sample only from this "nucleus"

### Top-P Values:
‚Ä¢ **0.1**: Very narrow selection (10% most likely)
‚Ä¢ **0.5**: Moderate diversity (50% most likely)
‚Ä¢ **0.9**: Wide selection (90% most likely)
‚Ä¢ **1.0**: Consider all tokens

## Top-K Sampling:
Limits selection to K most likely tokens.

### How it Works:
1. Select K highest probability tokens
2. Redistribute probabilities
3. Sample from this subset

### Top-K Values:
‚Ä¢ **1**: Always pick most likely (greedy)
‚Ä¢ **10**: Very focused
‚Ä¢ **40**: Balanced (default)
‚Ä¢ **100**: More variety
‚Ä¢ **‚àû**: No limit

## Combining Parameters:
Best results often come from combining:
‚Ä¢ Temperature: 0.7
‚Ä¢ Top-P: 0.9
‚Ä¢ Top-K: 40

This provides controlled creativity without chaos!
        `,
        quiz: [
          {
            question: 'What does Top-P=0.9 mean?',
            options: [
              'Use 90 tokens',
              'Sample from tokens covering 90% cumulative probability',
              'Set temperature to 0.9',
              'Use 90% of context window'
            ],
            correctAnswer: 1,
            explanation: 'Top-P=0.9 means sampling from tokens that together represent 90% of the probability mass.'
          }
        ]
      }
    ]
  },
  {
    id: 'embeddings',
    title: 'üî¢ Chapter 3: Embeddings & Vectors',
    description: 'Understanding text embeddings and vector representations',
    lessons: [
      {
        id: 'what-are-embeddings',
        title: 'What are Embeddings?',
        description: 'Converting text to numbers',
        content: `
# Text Embeddings

## Definition:
Embeddings are numerical representations of text that capture semantic meaning.

## How Embeddings Work:

### 1. Text to Vector
"Hello world" ‚Üí [0.23, -0.45, 0.67, ..., 0.12]

### 2. Dimensions
‚Ä¢ GPT-OSS 20: 4,096 dimensions
‚Ä¢ GPT-OSS 120: 8,192 dimensions
‚Ä¢ Each dimension captures different aspects

### 3. Semantic Similarity
Similar meanings = Similar vectors
‚Ä¢ "dog" ‚âà "puppy" (high similarity)
‚Ä¢ "dog" ‚â† "car" (low similarity)

## Applications:

### Semantic Search
Find documents by meaning, not keywords

### Clustering
Group similar texts together

### Classification
Categorize text based on embeddings

### Recommendation
Find similar content

## Vector Operations:

### Cosine Similarity
Measures angle between vectors (0-1 scale)
‚Ä¢ 1.0 = Identical meaning
‚Ä¢ 0.5 = Somewhat related
‚Ä¢ 0.0 = Unrelated

### Euclidean Distance
Measures straight-line distance
‚Ä¢ Smaller = More similar
        `,
        examples: [
          'Text: "Machine learning" ‚Üí Vector: [0.34, -0.21, ...]',
          'Similarity("cat", "kitten") = 0.89',
          'Similarity("cat", "airplane") = 0.12'
        ]
      }
    ]
  },
  {
    id: 'fine-tuning',
    title: 'üéØ Chapter 4: Fine-Tuning & Training',
    description: 'Customizing models for specific tasks',
    lessons: [
      {
        id: 'fine-tuning-basics',
        title: 'Fine-Tuning Fundamentals',
        description: 'Adapting models to your needs',
        content: `
# Fine-Tuning Language Models

## What is Fine-Tuning?
Taking a pre-trained model and training it further on specific data.

## Why Fine-Tune?

### 1. Domain Specialization
Train on medical, legal, or technical texts

### 2. Task Optimization
Improve performance on specific tasks

### 3. Style Adaptation
Match writing style or tone

### 4. Knowledge Injection
Add proprietary or recent information

## Fine-Tuning Process:

### Step 1: Data Preparation
‚Ä¢ Collect domain-specific examples
‚Ä¢ Format as input-output pairs
‚Ä¢ Clean and validate data

### Step 2: Configuration
‚Ä¢ Learning rate: 1e-5 to 5e-5
‚Ä¢ Batch size: Based on GPU memory
‚Ä¢ Epochs: 3-5 typically

### Step 3: Training
‚Ä¢ GPT-OSS 20: 2-4 hours on GPU
‚Ä¢ GPT-OSS 120: 8-12 hours on GPU

### Step 4: Evaluation
‚Ä¢ Test on validation set
‚Ä¢ Compare to base model
‚Ä¢ Check for overfitting

## Best Practices:
‚úì Start with small learning rates
‚úì Use at least 100 examples
‚úì Monitor training loss
‚úì Keep validation set separate
        `
      }
    ]
  },
  {
    id: 'prompting',
    title: '‚úçÔ∏è Chapter 5: Prompt Engineering',
    description: 'Mastering the art of prompting',
    lessons: [
      {
        id: 'prompt-techniques',
        title: 'Advanced Prompting Techniques',
        description: 'Getting better results from LLMs',
        content: `
# Prompt Engineering Techniques

## 1. Zero-Shot Prompting
Direct instruction without examples:
"Translate 'Hello' to Spanish"

## 2. Few-Shot Prompting
Provide examples before the task:
\`\`\`
English: Hello ‚Üí Spanish: Hola
English: Goodbye ‚Üí Spanish: Adi√≥s
English: Thank you ‚Üí Spanish: 
\`\`\`

## 3. Chain-of-Thought (CoT)
Ask model to explain reasoning:
"Let's solve this step by step..."

## 4. Role-Playing
Assign a persona:
"You are an expert Python developer..."

## 5. Structured Output
Request specific format:
"Respond in JSON format with keys: summary, sentiment, keywords"

## Prompt Components:

### Context
Background information

### Instruction
What you want done

### Input
The data to process

### Output Format
How to structure response

## Tips for Better Prompts:
‚úì Be specific and clear
‚úì Provide context
‚úì Use examples
‚úì Specify format
‚úì Iterate and refine
        `,
        examples: [
          'Bad: "Write about dogs"',
          'Good: "Write a 200-word educational paragraph about golden retriever training techniques for first-time owners"'
        ]
      }
    ]
  }
];

export function getLessonById(chapterId: string, lessonId: string): Lesson | undefined {
  const chapter = AI_CURRICULUM.find(c => c.id === chapterId);
  return chapter?.lessons.find(l => l.id === lessonId);
}

export function searchLessons(query: string): Lesson[] {
  const results: Lesson[] = [];
  const searchTerm = query.toLowerCase();
  
  for (const chapter of AI_CURRICULUM) {
    for (const lesson of chapter.lessons) {
      if (
        lesson.title.toLowerCase().includes(searchTerm) ||
        lesson.description.toLowerCase().includes(searchTerm) ||
        lesson.content.toLowerCase().includes(searchTerm)
      ) {
        results.push(lesson);
      }
    }
  }
  
  return results;
}