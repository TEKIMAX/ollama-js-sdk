---
title: 'Getting Started with Ollama SDK'
description: 'Learn how to use the TypeScript/JavaScript SDK for Ollama'
---

# Getting Started with Ollama SDK

The `@tekimax/ollama-sdk` is a comprehensive TypeScript/JavaScript SDK for interacting with Ollama, an API for running large language models locally. We created this SDK specifically to simplify LLM integration for workshops, tutorials, and educational purposes.

## Why We Built This SDK

This SDK was designed with education in mind:

- **Simplified LLM Integration**: Makes working with large language models accessible for developers of all experience levels
- **Workshop-Ready**: Includes ready-to-use examples and exercises for LLM workshops
- **Interactive Tutorials**: Comes with interactive tutorials to help you learn LLM concepts
- **Local First**: Focuses on running models locally with Ollama for privacy and learning

## What is Ollama?

[Ollama](https://ollama.ai) lets you run open-source large language models, such as Llama 2, locally on your machine. This gives you the power of AI models without sending your data to external APIs.

## What can you do with this SDK?

With the Ollama SDK, you can:

- Generate text responses from prompts
- Stream responses in real-time
- Create embeddings for semantic search
- Manage and interact with different models
- Use OpenAI-compatible interfaces with local models
- Leverage tool-calling capabilities (function calling)

## Included Workshops & Tutorials

This SDK includes:

1. **LLM Basics Workshop**: A step-by-step introduction to working with language models
2. **Semantic Search Tutorial**: Learn how to implement semantic search with embeddings
3. **Tool Calling Workshop**: Explore how to extend LLM capabilities with external tools
4. **Interactive Chat Example**: Build a chat application with streaming responses

## Requirements

- **Node.js**: v16.0.0 or higher
- **Ollama**: Running locally or on a remote server
- **TypeScript** (optional): For type safety

## Installation

You can install the SDK using npm, yarn, or pnpm:

```bash
# Using npm
npm install @tekimax/ollama-sdk

# Using yarn
yarn add @tekimax/ollama-sdk

# Using pnpm
pnpm add @tekimax/ollama-sdk
```

## Quick Example

Here's a simple example to get you started:

```javascript
import { OllamaKit } from '@tekimax/ollama-sdk';

async function main() {
  // Initialize the client (default: http://localhost:11434)
  const ollama = new OllamaKit();
  
  // Generate text with a model
  const response = await ollama.generate({
    model: 'llama2',
    prompt: 'Explain quantum computing in simple terms',
    temperature: 0.7
  });
  
  console.log('Generated response:');
  console.log(response.response);
}

main().catch(console.error);
```

## Command & Query Interface

Our SDK features a simple, intuitive command and query interface:

- **Commands**: Execute actions (like pulling models)
  ```javascript
  // Pull a model
  await ollama.pullModel({ name: 'llama2' });
  ```

- **Queries**: Request information (like generating text)
  ```javascript
  // Generate text (query)
  const response = await ollama.generate({ 
    model: 'llama2',
    prompt: 'What is AI?'
  });
  ```

## Workshop Tutorial

Check out our included workshop documentation to get hands-on experience with LLMs:
- [Workshop Setup](/docs/workshop-setup.md) - Set up your environment
- [Workshop Guide](/docs/workshop.md) - Follow our step-by-step guide

## Next Steps

- [Installation](/docs/installation) - Detailed installation instructions
- [Quick Start](/docs/quickstart) - More examples to get you started quickly
- [API Reference](/docs/api/overview) - Complete API documentation
- [Guides](/docs/guides/basic-usage) - In-depth guides for specific use cases 